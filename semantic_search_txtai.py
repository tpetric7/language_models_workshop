# -*- coding: utf-8 -*-
"""Semantic Search txtai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zuNGwHWwtz6geNSPcfBWW4auo-C8pI1h

# Semantično iskanje v slovenskih besedilih

## Namestitev Pythonovih knjižnic
"""

!pip install txtai[all] sentencepiece sacremoses fasttext torch torchvision

"""## Besedila"""

# Sample data for indexing
data = [
  "Število novorojenih otrok v naši državi se je v tem letu po večletnem zaskrbljujočem nazadovanju končno spet povečalo",
  "Po poplavah v Nemčiji je tudi Slovenija doživela katastrofalno povodenj, ki je zajela tretjino države",
  "Upokojen gradbeni delavec je v loteriji dobil 100.000 evrov",
  "V tem stoletju pričakujemo dvig zračne temperature za 2 stopnji Celzija",
  "V Sudanu je bilo leta 2017 več kot 100.000 smrtnih žrtev",

]

"""## Semantično indeksiranje

Ustvarimo vektorsko reprezentacijo besedil, kar omogoča semantično iskanje.
"""

from txtai import Embeddings

embeddings = Embeddings(path="cjvt/sloberta-trendi-topics")
embeddings.index(data)

print("Semantic Search Results:")
for query in ["feel good story", "climate change"]:
    uid = embeddings.search(query, 1)[0][0]
    print(f"Query: {query}, Result: {data[uid]}")

"""Še preizkus modela za ustvarjanje vektorske reprezentacije besedil s slovenskimi gesli."""

from txtai import Embeddings
embeddings = Embeddings(path="cjvt/sloberta-trendi-topics")
embeddings.index(data)

print("Semantic Search Results:")
for query in ["vesel dogodek", "podnebne spremembe"]:
    results = embeddings.search(query, len(data))
    print(f"Query: {query}")
    for uid, score in results:
        # Adjust the threshold value as needed
        if score > 0.72:
            print(data[uid])

"""### Semantično iskanje v naloženem besedilu

V tretjem preizkusu naložimo besedilo (npr. "Zakisljevanje oceanov.txt"), da bi v njem poiskali povedi o določeni temi (npr. podnebne spremembe).
"""

from txtai import Embeddings
import os
from nltk.tokenize import sent_tokenize
import nltk

# Ensure nltk resources are available
nltk.download('punkt')
nltk.download('punkt_tab')

# Filepath to the speech
filepath = "/content/Antrittsrede_von_Donald_Trump_englisch.txt"

# Read the content of the file
with open(filepath, 'r', encoding='utf-8') as file:
    data = file.read()  # Read entire file content

# Sentence tokenization
data = sent_tokenize(data)  # Tokenizes the content into sentences

# Create embeddings instance
embeddings = Embeddings(path="intfloat/multilingual-e5-large")
embeddings.index(data)  # Index the tokenized sentences

print("Semantic Search Results:")
for query in ["gender policy", "illegal immigration", "climate policy"]:
    results = embeddings.search(query, len(data))
    print(f"Query: {query}")
    for uid, score in results:
        # Adjust the threshold value as needed
        if score > 0.795:
            print(data[uid])

"""### Semantično iskanje v več besedilih"""

from txtai import Embeddings
import os
from nltk.tokenize import sent_tokenize
import nltk

nltk.download('punkt')
nltk.download('punkt_tab')

directory_path = "/content/"

all_sentences = []

for filename in os.listdir(directory_path):
    file_path = os.path.join(directory_path, filename)
    if os.path.isfile(file_path):
        with open(file_path, 'r', encoding='utf-8') as file:
            file_data = file.read()
            tokenized_sentences = sent_tokenize(file_data)
            all_sentences.extend(tokenized_sentences)

# Remove duplicate sentences
all_sentences = list(set(all_sentences))

embeddings = Embeddings(path="intfloat/multilingual-e5-large")
embeddings.index(all_sentences)

print("Semantic Search Results:")
with open("output_mixed.txt", "w", encoding="utf-8") as f:
    for query in ["gender policy", "illegal immigration", "climate policy"]:
        results = embeddings.search(query, 50)  # Limit number of results
        seen_results = set()  # Track unique results
        f.write(f"Query: {query}\n")
        print(f"Query: {query}")
        for uid, score in results:
            if score > 0.795 and all_sentences[uid] not in seen_results:
                seen_results.add(all_sentences[uid])
                print(all_sentences[uid])
                f.write(all_sentences[uid] + "\n")