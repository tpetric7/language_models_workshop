# -*- coding: utf-8 -*-
"""R_Py_bertopicr_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RnyYAHuweZv5w8K9z-fX9naVHCona0nm

# Topic analiza

## Vzpostavitev Pythonovega okolja
"""

# If needed, run shell commands in R via system()
system("which python")
system("python --version")
system("sudo apt-get update -y")
system("sudo apt-get install -y python3-dev")
system("sudo apt-get install -y python3-venv")

"""## Namestitev vmesnika za Python"""

install.packages("reticulate")
library(reticulate)

"""## Ustvarjanje Pythonovega virtualnega okolja"""

# virtualenv_create("bertopic", python = "/usr/local/bin/python")
system("python3 -m venv /root/.virtualenvs/bertopic")
system("pip install virtualenv")
system("virtualenv /root/.virtualenvs/bertopic")

"""# Namestitev Pythonovih knjižnic"""

library(reticulate)
use_virtualenv("bertopic", required = TRUE)
py_install("pandas", envname = "bertopic")
py_install(c("numpy", "matplotlib"), envname = "bertopic")
py_install("bertopic", envname = "bertopic")
py_install("umap-learn", envname = "bertopic")
py_install("hdbscan", envname = "bertopic")
py_install("scikit-learn", envname = "bertopic")
py_install("plotly", envname = "bertopic")

"""## Priklic Pythonovih knjižnic"""

# Import necessary Python modules
np <- import("numpy")
umap <- import("umap")
UMAP <- umap$UMAP
hdbscan <- import("hdbscan")
HDBSCAN <- hdbscan$HDBSCAN
sklearn <- import("sklearn")
CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer
bertopic <- import("bertopic")
plotly <- import("plotly")
datetime <- import("datetime")

"""## Namestitev knjižnic R"""

install.packages(c("dplyr", "tidyr", "tictoc", "htmltools", "htmlwidgets"))

install.packages("devtools")

devtools::install_github("tpetric7/bertopicr")

"""## Priklic knjižnic R"""

library(dplyr)
library(tidyr)
# library(quanteda)
library(tictoc)
library(htmltools)
library(htmlwidgets)
# library(arrow)
library(bertopicr)

"""## Branje podatkovnega niza"""

input_folder <- "/content/"
input_file <- "ucb_uni_cleaned.rds"
dataset <- readRDS(file.path(input_folder, input_file))
names(dataset)
dim(dataset)

"""## Branje nezaželenih besed"""

input_folder <- "/content/"
input_file <- "all_stopwords.txt"
all_stopwords <- read_lines(file.path(input_folder, input_file))

"""## Izbor besedilnega stolpca"""

texts_cleaned = dataset$text_clean
# titles = dataset$doc_id
titles = dataset$Doc_ID
# timestamps <- as.list(dataset$date)
timestamps <- as.integer(dataset$year)

texts_cleaned[[1]]

"""## Vektorska reprezentacija (embeddings)"""

# Embed the sentences
py <- import_builtins()
sentence_transformers <- import("sentence_transformers")
SentenceTransformer <- sentence_transformers$SentenceTransformer
# embedding_model = SentenceTransformer("BAAI/bge-m3")
embedding_model = SentenceTransformer("cjvt/sloberta-trendi-topics")
embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)

"""## Redukcija dimenzij in grozdenje"""

# Initialize UMAP and HDBSCAN models
umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)

hdbscan_model <- HDBSCAN(min_cluster_size=50L, min_samples = 20L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)

# Initialize CountVectorizer
vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L),
                                    max_features = 10000L, max_df = 50L,
                                    stop_words = all_stopwords)
sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned)
sentence_vectors_dense <- np$array(sentence_vectors)
sentence_vectors_dense <- py_to_r(sentence_vectors_dense)

"""## Reprezentacijski modeli"""

# Initialize representation models
keybert_model <- bertopic$representation$KeyBERTInspired()
# openai <- import("openai")
# OpenAI <- openai$OpenAI
# ollama <- import("ollama")

# # Point to the local server (ollama or lm-studio)
# client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama')
# # client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')

# prompt <- "
# I have a topic that contains the following documents:
# [DOCUMENTS]
# The topic is described by the following keywords: [KEYWORDS]

# Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:
# topic: <topic label>
# "

# # download an appropriate local LLM for your computer with ollama/lm-studio
# openai_model <- bertopic$representation$OpenAI(client,
#                                                model = "llama3.1:8b-instruct-fp16",
#                                                exponential_backoff = TRUE,
#                                                chat = TRUE,
#                                                prompt = prompt)

# # downlaod a spacy language model from spacy.io before use here
# pos_model <- bertopic$representation$PartOfSpeech("sl_core_news_trf")

# diversity set relatively high to reduce repetition of keyword word forms
mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)

# Combine all representation models
representation_model <- list(
  "KeyBERT" = keybert_model,
  # "OpenAI" = openai_model,
  # "POS" = pos_model,
  "MMR" = mmr_model
)

"""## Vzpostavitev modela BERTopic"""

# Initialize BERTopic model with pipeline models and hyperparameters
BERTopic <- bertopic$BERTopic
topic_model <- BERTopic(
  embedding_model = embedding_model,
  umap_model = umap_model,
  hdbscan_model = hdbscan_model,
  vectorizer_model = vectorizer_model,
  # zeroshot_topic_list = zeroshot_topic_list,
  # zeroshot_min_similarity = 0.6, # 0.85
  representation_model = representation_model,
  calculate_probabilities = TRUE,
  top_n_words = 10L,
  verbose = TRUE
)

"""## Učenje modela BERTopic"""

tictoc::tic()

# Fit the model and transform the texts
fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings)
topics <- fit_transform[[1]]
# probs <- fit_transform[[2]]

# Now transform the texts to get the updated probabilities
transform_result <- topic_model$transform(texts_cleaned)
probs <- transform_result[[2]]  # Extract the updated probabilities

tictoc::toc()

"""## Časovna dimenzija"""

# Converting R Date to Python datetime
datetime <- import("datetime")

# timestamps <- as.list(dataset$date)
timestamps <- as.integer(dataset$year)

# # Convert each R date object to an ISO 8601 string
# timestamps <- lapply(timestamps, function(x) {
#   format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
# })

# Dynamic topic model
topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)

"""## Rezultati topic analize"""

# Combine results with additional columns
results <- dataset |>
  mutate(Topic = topics,
         Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence

results <- results |>
  mutate(row_id = row_number()) |>
  select(row_id, everything())

head(results,10)

"""## Informacije o dokumentih"""

library(bertopicr)
document_info_df <- get_document_info_df(model = topic_model,
                                         texts = texts_cleaned,
                                         drop_expanded_columns = TRUE)
document_info_df |> head()

# Create a data frame similar to df_docs
df_docs <- tibble(Topic = results$Topic,
                  Document = results$text_clean,
                  probs = results$Probability)
rep_docs <- get_most_representative_docs(df = df_docs,
                                         topic_nr = 3,
                                         n_docs = 5)
unique(rep_docs)

"""## Informacije o temah (topics)"""

topic_info_df <- get_topic_info_df(model = topic_model,
                                   drop_expanded_columns = TRUE)
head(topic_info_df)

topics_df <- get_topics_df(model = topic_model)
head(topics_df, 10)

"""## Interaktivni stolpčni diagram"""

visualize_barchart(model = topic_model,
                   filename = "topics_topwords_interactive_barchart.html", # default
                   open_file = FALSE) # TRUE enables output in browser

"""## Iskanje teme (topics)"""

find_topics_df(model = topic_model,
               queries = "podnebne spremembe", # user input
               top_n = 10, # default
               return_tibble = TRUE) # default

find_topics_df(model = topic_model,
                               queries = c("podnebne spremembe", "trajnostni razvoj"),
                               top_n = 5)

"""## Pridobivanje ključnih besed za temo (topic)"""

get_topic_df(model = topic_model,
                           topic_number = 0,
                           top_n = 5, # default is 10
                           return_tibble = TRUE) # default

"""## Interaktivna vizualizacija distribucije tem (topics)"""

# default filename: topic_dist_interactive.html
visualize_distribution(model = topic_model,
                       text_id = 1, # user input
                       probabilities = probs) # see model training

"""## Interaktivna vizualizacija razdalje med temami (topics)"""

visualize_topics(model = topic_model,
                 filename = "intertopic_distance_map") # default name

"""## Podobnost tem (topics)"""

visualize_heatmap(model = topic_model,
                  filename = "topics_similarity_heatmap",
                  auto_open = FALSE)

"""## Hierarhija tem (topics)"""

visualize_hierarchy(model = topic_model,
                    hierarchical_topics = NULL, # default
                    filename = "topic_hierarchy", # default name, html extension
                    auto_open = FALSE) # TRUE enables output in browser

hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned)
visualize_hierarchy(model = topic_model,
                    hierarchical_topics = hierarchical_topics,
                    filename = "topic_hierarchy", # default name, html extension
                    auto_open = FALSE) # TRUE enables output in browser

"""## Interaktivna vizualazacija dokumentov 2D in 3D"""

# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)

visualize_documents(model = topic_model,
                    texts = texts_cleaned,
                    reduced_embeddings = reduced_embeddings,
                    filename = "visualize_documents", # default extension html
                    auto_open = FALSE) # TRUE enables output in browser

# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 3L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)

visualize_documents_3d(model = topic_model,
                       texts = texts_cleaned,
                       reduced_embeddings = reduced_embeddings,
                       custom_labels = FALSE, # default
                       hide_annotation = TRUE, # default
                       tooltips = c("Topic", "Name", "Probability", "Text"), # deault
                       filename = "visualize_documents_3d", # default name
                       auto_open = FALSE) # TRUE enables output in browser

"""## Časovna razporeditev tem (topics)"""

visualize_topics_over_time(model = topic_model,
                           # see Topic Dynamics section above
                           topics_over_time_model = topics_over_time,
                           top_n_topics = 10, # default is 20
                           filename = "topics_over_time") # default, html extension

"""## Razporeditev tem (topics) po skupinah"""

classes = as.list(dataset$veda) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)

visualize_topics_per_class(model = topic_model,
                           topics_per_class = topics_per_class,
                           start = 0, # default
                           end = 10, # default
                           filename = "topics_per_class", # default, html extension
                           auto_open = FALSE) # TRUE enables output in browser

"""## Shrani / download interaktivne vizualizacije"""

# prompt: download all html files in the /content/ folder to the local computer

# This code requires the "utils" package which is a base R package
# so you shouldn't need to install it

# List all HTML files in the /content directory
html_files <- list.files(path = "/content/", pattern = "\\.html$", full.names = TRUE)

print(html_files)

# Check if there are any HTML files
if (length(html_files) > 0) {
  # Download each HTML file
  for (file in html_files) {
    download.file(file, destfile = file)
    # Print a message to indicate the download is complete
    cat(paste("Downloaded:", file, "to", local_file, "\n"))
  }
} else {
  # Print a message if no HTML files are found
  message("No HTML files found in the /content directory.")
}