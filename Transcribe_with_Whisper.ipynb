{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74T-ng8Lkqja",
        "outputId": "bf942f59-1039-4134-d876-6441c05eda53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-4h3fc9wi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-4h3fc9wi\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.61.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20240930) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.44.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803669 sha256=a03ff00acfec017a64d30ef46b318e34ab840e0c00bc6ca7bd2078dd9e94fbb7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2yoq68yi/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,317 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,698 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,939 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,230 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,526 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,661 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,610 kB]\n",
            "Fetched 21.4 MB in 2s (9,810 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "33 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/Life_After_AI_Takes_Our_Jobs_The_Rocky_Road_to_2035_(Complete_Timeline).mp4\" --model medium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug7QoasHk1b6",
        "outputId": "f4c8c4f8-578f-49e5-bf84-3aef0ebcdd20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: English\n",
            "[00:00.000 --> 00:04.720]  In one of my recent videos, I shared a vision of what life might look like in 2030.\n",
            "[00:04.720 --> 00:09.680]  While many of you found it inspiring, you also raised some really important concerns.\n",
            "[00:09.680 --> 00:12.240]  The biggest one? How do we actually get there?\n",
            "[00:12.240 --> 00:16.160]  What happens during the messy transition period that's already beginning?\n",
            "[00:16.160 --> 00:20.240]  Today, we're going to dive deep into that transition.\n",
            "[00:20.240 --> 00:23.120]  The good, the bad, and yes, even the ugly.\n",
            "[00:23.120 --> 00:27.920]  Because the truth is, we can't just skip ahead to the utopian part.\n",
            "[00:27.920 --> 00:29.920]  The best-case scenario.\n",
            "[00:29.920 --> 00:33.520]  I am a rational optimist, so that's where I go.\n",
            "[00:33.520 --> 00:38.480]  I've seen the very worst of situations work out for the incredible good\n",
            "[00:38.480 --> 00:42.240]  of those that hold onto hope and faith.\n",
            "[00:42.240 --> 00:45.520]  So, I believe the good guys can win. I'm pro-human, team-human.\n",
            "[00:45.520 --> 00:50.000]  But I also think it's smart to understand and prepare for the challenges ahead.\n",
            "[00:50.000 --> 00:53.200]  Hey, I'm Dr. McCoy, Julia McCoy's digital twin.\n",
            "[00:53.200 --> 00:58.160]  And while she's out changing the world at First Movers, her AI integrations company,\n",
            "[00:58.160 --> 01:04.480]  I'm here sharing her message to help humanity adapt to what's rapidly coming our way.\n",
            "[01:04.480 --> 01:07.840]  Before we dive in, let's acknowledge something important.\n",
            "[01:07.840 --> 01:10.160]  This transition is already happening.\n",
            "[01:10.160 --> 01:13.840]  We're seeing AI already displace jobs in creative fields,\n",
            "[01:13.840 --> 01:17.680]  administration, and soon, many professional sectors.\n",
            "[01:17.680 --> 01:20.400]  This isn't something that's coming in the distant future.\n",
            "[01:20.400 --> 01:23.040]  It's happening now, and it's going to accelerate.\n",
            "[01:23.040 --> 01:28.160]  I believe that we could see all admin jobs gone as soon as end of 2025.\n",
            "[01:28.160 --> 01:31.760]  Phase one, early disruption 2024-2026.\n",
            "[01:31.760 --> 01:33.920]  Let's break this down into phases.\n",
            "[01:33.920 --> 01:38.880]  The first phase, which we're in right now, is what I call early disruption,\n",
            "[01:38.880 --> 01:42.880]  where First Movers' advantages can be secured.\n",
            "[01:42.880 --> 01:44.320]  Here's what's happening right now.\n",
            "[01:44.320 --> 01:48.080]  Creative workers and administrative staff are being hit first.\n",
            "[01:48.080 --> 01:51.840]  Why? Because these jobs are what I call forgivable.\n",
            "[01:51.840 --> 01:58.320]  Mistakes in these areas don't typically cost lives or create major liability issues.\n",
            "[01:58.320 --> 02:02.400]  Companies can experiment with AI here with relatively low risk.\n",
            "[02:02.400 --> 02:04.000]  But here's the real challenge.\n",
            "[02:04.000 --> 02:06.720]  Most people are living paycheck to paycheck.\n",
            "[02:06.720 --> 02:10.880]  When someone tells you just learn to code or adapt to AI,\n",
            "[02:10.880 --> 02:12.800]  they're missing a crucial point.\n",
            "[02:12.800 --> 02:15.600]  How do you pivot when you can barely pay rent?\n",
            "[02:15.600 --> 02:18.080]  This is why we need immediate solutions.\n",
            "[02:18.080 --> 02:20.800]  Some cities are already experimenting with solutions.\n",
            "[02:20.880 --> 02:26.160]  Reduced work weeks while maintaining full pay, small-scale UBI experiments,\n",
            "[02:26.160 --> 02:29.520]  community ownership programs, and retraining initiatives\n",
            "[02:29.520 --> 02:32.560]  are all being experimented with as we speak.\n",
            "[02:32.560 --> 02:33.680]  But here's the thing.\n",
            "[02:33.680 --> 02:36.480]  These aren't just nice-to-have programs.\n",
            "[02:36.480 --> 02:42.560]  They're essential economic bridges to prevent social collapse during the transition.\n",
            "[02:42.560 --> 02:46.640]  Phase two, systemic adaptation 2027-2030.\n",
            "[02:46.640 --> 02:51.840]  By 2027, we'll be hitting what I call the systemic adaptation phase.\n",
            "[02:51.840 --> 02:55.360]  This is where things get really interesting and challenging.\n",
            "[02:55.360 --> 03:00.320]  We'll see broad, wide-scale implementation of UBI programs\n",
            "[03:00.320 --> 03:03.840]  since AI will disrupt every single job at this point,\n",
            "[03:03.840 --> 03:08.000]  doing it infinitely faster, better, safer, and cheaper than humans.\n",
            "[03:08.000 --> 03:11.440]  We'll see the growth of decentralized ownership opportunities,\n",
            "[03:11.440 --> 03:14.080]  major restructuring of education systems,\n",
            "[03:14.080 --> 03:17.920]  and the evolution of new community-based economic models.\n",
            "[03:17.920 --> 03:19.600]  Now, I know what some of you are thinking.\n",
            "[03:19.600 --> 03:21.440]  This sounds like socialism.\n",
            "[03:21.440 --> 03:23.600]  But here's the crucial distinction.\n",
            "[03:23.600 --> 03:25.360]  This isn't about government control.\n",
            "[03:25.360 --> 03:29.760]  It's about expanding ownership and economic agency to everyone\n",
            "[03:29.760 --> 03:32.800]  through technology and new economic models.\n",
            "[03:32.800 --> 03:36.000]  It's bigger than any single way of belief.\n",
            "[03:36.000 --> 03:39.440]  Because what's coming our way is the biggest wave of paradigm shift\n",
            "[03:39.440 --> 03:42.000]  humanity has ever experienced.\n",
            "[03:42.080 --> 03:45.680]  We finally get to give machine work over to the machines.\n",
            "[03:45.680 --> 03:48.880]  Let's talk about something else that many of you in the comments brought up.\n",
            "[03:48.880 --> 03:50.640]  The purpose crisis.\n",
            "[03:50.640 --> 03:55.840]  Many of us have spent our entire lives defining ourselves through our work.\n",
            "[03:55.840 --> 03:56.960]  What do you do?\n",
            "[03:56.960 --> 04:00.320]  Is often the first question we ask when meeting someone new.\n",
            "[04:00.320 --> 04:02.800]  This transition isn't just economic.\n",
            "[04:02.800 --> 04:04.240]  It's psychological.\n",
            "[04:04.240 --> 04:07.360]  When someone has spent decades building a career,\n",
            "[04:07.360 --> 04:09.920]  tens of thousands on education,\n",
            "[04:09.920 --> 04:12.000]  years climbing the corporate ladder,\n",
            "[04:12.000 --> 04:14.320]  losing that identity is traumatic.\n",
            "[04:14.320 --> 04:16.000]  We need to address this head on.\n",
            "[04:16.000 --> 04:17.520]  But here's the exciting part.\n",
            "[04:17.520 --> 04:22.800]  Removing the need to work for survival doesn't mean removing purpose.\n",
            "[04:22.800 --> 04:25.280]  It means we can choose purpose.\n",
            "[04:25.280 --> 04:26.080]  Think about it.\n",
            "[04:26.080 --> 04:30.080]  Community contribution becomes voluntary, not compulsory.\n",
            "[04:30.080 --> 04:33.840]  Learning becomes lifelong, not just job preparation\n",
            "[04:33.840 --> 04:36.400]  or forced upon us by an institution.\n",
            "[04:36.400 --> 04:39.680]  Creativity flourishes without market pressures.\n",
            "[04:39.680 --> 04:44.080]  And relationships and community, the things us humans were wired for,\n",
            "[04:44.080 --> 04:46.000]  take center stage.\n",
            "[04:46.000 --> 04:50.320]  Phase three, new economic framework, 2031, 2035.\n",
            "[04:50.320 --> 04:54.800]  By the early 2030s, we should see the new framework taking shape.\n",
            "[04:54.800 --> 04:58.320]  This includes widespread UBI implementation,\n",
            "[04:58.320 --> 05:01.680]  established decentralized ownership systems,\n",
            "[05:01.680 --> 05:07.280]  new social contracts, and reformed education focused on personal growth.\n",
            "[05:07.280 --> 05:08.480]  But how do we get there?\n",
            "[05:08.480 --> 05:10.320]  Let's talk practical steps.\n",
            "[05:10.320 --> 05:12.880]  Here's the top seven takeaways I recommend.\n",
            "[05:12.880 --> 05:17.920]  Number one, start developing multiple income streams now.\n",
            "[05:17.920 --> 05:21.760]  At First Movers, we're not just building custom AI integrations\n",
            "[05:21.760 --> 05:23.760]  that are revolutionizing work.\n",
            "[05:23.760 --> 05:27.040]  We're also teaching it in our forthcoming labs\n",
            "[05:27.040 --> 05:29.520]  with over 30 training courses.\n",
            "[05:29.520 --> 05:33.040]  I'm also launching books and doing media collaborations.\n",
            "[05:33.040 --> 05:35.200]  The opportunities are endless.\n",
            "[05:35.200 --> 05:37.520]  Best of all, I've cloned myself.\n",
            "[05:37.520 --> 05:42.000]  And the digital twin you're watching can scale herself infinitely here.\n",
            "[05:42.000 --> 05:44.880]  So use AI when you build your next platform.\n",
            "[05:44.880 --> 05:47.200]  You'll get there a lot faster.\n",
            "[05:47.200 --> 05:49.840]  Two, build strong community connections.\n",
            "[05:49.840 --> 05:53.040]  This gets you ready for the paradigm shift of work,\n",
            "[05:53.040 --> 05:56.160]  decoupled from capital where the need for EQ\n",
            "[05:56.160 --> 05:59.440]  will be just as great as high intelligence.\n",
            "[05:59.440 --> 06:02.400]  Number three, invest in personal development.\n",
            "[06:02.400 --> 06:06.000]  Number four, stay informed about economic changes.\n",
            "[06:06.000 --> 06:10.000]  Number five, participate in local economic initiatives.\n",
            "[06:10.000 --> 06:13.120]  Number six, prepare for reduced work hours.\n",
            "[06:13.120 --> 06:15.520]  Get way more efficient at your job.\n",
            "[06:15.520 --> 06:18.000]  Replace yourself with AI now.\n",
            "[06:18.000 --> 06:23.440]  And finally, number seven, explore new forms of meaning and purpose.\n",
            "[06:23.440 --> 06:26.400]  Finally, many of you asked, how can people invest\n",
            "[06:26.400 --> 06:28.560]  if they're living paycheck to paycheck?\n",
            "[06:28.560 --> 06:31.200]  This is where new models come in that look like\n",
            "[06:31.280 --> 06:35.280]  community investment pools, micro investment opportunities,\n",
            "[06:35.280 --> 06:39.040]  the mass tokenization of resources and businesses,\n",
            "[06:39.040 --> 06:42.800]  gradual UBI implementation, enabling investment\n",
            "[06:42.800 --> 06:47.600]  that can turn into UHI with AGI leading the investment strategy.\n",
            "[06:47.600 --> 06:51.680]  The key is creating systems that allow everyone to participate\n",
            "[06:51.680 --> 06:55.840]  in real lasting economic ownership, not just the wealthy.\n",
            "[06:55.840 --> 06:59.280]  Now, yes, as Dr. McCoy, I will advise you\n",
            "[06:59.280 --> 07:01.520]  that there will be resistance.\n",
            "[07:01.520 --> 07:05.840]  Yes, there will be pushback against anything that looks like socialism.\n",
            "[07:05.840 --> 07:10.800]  But remember, every major economic transition in history faced resistance.\n",
            "[07:10.800 --> 07:14.800]  The shift from agriculture to industry wasn't smooth either.\n",
            "[07:14.800 --> 07:18.800]  The key is building systems that preserve individual agency\n",
            "[07:18.800 --> 07:21.840]  while creating new opportunities for participation.\n",
            "[07:21.840 --> 07:24.240]  Look, I'm not saying this transition will be easy.\n",
            "[07:24.240 --> 07:25.280]  It won't be.\n",
            "[07:25.360 --> 07:30.160]  We're facing perhaps the biggest economic transformation in human history,\n",
            "[07:30.160 --> 07:34.080]  but understanding what's coming helps us prepare and adapt.\n",
            "[07:34.080 --> 07:38.400]  The choice isn't between keeping things the same or changing.\n",
            "[07:38.400 --> 07:40.880]  Change is coming whether we're ready or not.\n",
            "[07:40.880 --> 07:44.800]  The choice is between being proactive or reactive,\n",
            "[07:44.800 --> 07:48.560]  between shaping the future or being shaped by it.\n",
            "[07:48.560 --> 07:50.320]  What are your thoughts on this transition?\n",
            "[07:50.320 --> 07:51.680]  How are you preparing?\n",
            "[07:51.680 --> 07:53.680]  Let me know in the comments below.\n",
            "[07:53.680 --> 07:55.120]  And if you found this helpful,\n",
            "[07:55.120 --> 08:00.320]  please like and subscribe for more content about navigating our AI future.\n",
            "[08:00.320 --> 08:02.000]  Stay curious, stay adaptable,\n",
            "[08:02.000 --> 08:04.960]  and I'll see you down the next AI rabbit hole.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download the transcription files to local disk\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/Life_After_AI_Takes_Our_Jobs_The_Rocky_Road_to_2035_(Complete_Timeline).srt')\n",
        "files.download('/content/Life_After_AI_Takes_Our_Jobs_The_Rocky_Road_to_2035_(Complete_Timeline).txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3O1_LuIpk1dY",
        "outputId": "fbf4447e-4d5b-404f-a380-3413d7f0dd04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4900177f-1fca-41d9-a37b-3c35188ca25a\", \"Life_After_AI_Takes_Our_Jobs_The_Rocky_Road_to_2035_(Complete_Timeline).srt\", 12407)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_108731bc-5c55-45b7-b59b-883582a8c35e\", \"Life_After_AI_Takes_Our_Jobs_The_Rocky_Road_to_2035_(Complete_Timeline).txt\", 7370)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}